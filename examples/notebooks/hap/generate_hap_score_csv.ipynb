{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAP Transform Example Notebook\n",
    "=====================================\n",
    "\n",
    "This notebook processes a CSV file containing text data to analyze for Hate, Abuse, and Profanity (HAP) scores.\n",
    "It converts the CSV file into Parquet format, uses the `hap_local_python.py` script to calculate HAP scores, \n",
    "and generates outputs for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Overview\n",
    "This notebook demonstrates the use of the HAP transformation to annotate documents with a `hap_score`, \n",
    "indicating the likelihood of Hate, Abuse, or Profanity in the text.\n",
    "\n",
    "### Workflow\n",
    "The HAP process consists of:\n",
    "1. **Sentence Splitting**: Documents are split into sentences using NLTK.\n",
    "2. **HAP Annotation**: Each sentence is scored between 0 and 1 (1 = high HAP, 0 = no HAP).\n",
    "3. **Aggregation**: The document's final HAP score is the maximum score among all sentences.\n",
    "\n",
    "\n",
    "### Configuration\n",
    "- **Model Name**: IBM Granite Guardian (`ibm-granite/granite-guardian-hap-38m` by default).\n",
    "- **Document Text Column** (`--doc_text_column`): Specify the input column containing document text to generate the hap_score against. Defaults to `contents`.\n",
    "- **Annotation Column** (`--annotation_column`): Specify the output column for HAP scores. Defaults to `hap_score`.\n",
    "\n",
    "\n",
    "### Steps in This Notebook\n",
    "1. Define paths and import libraries.\n",
    "2. Convert CSV input to Parquet.\n",
    "3. Run the HAP transformation script.\n",
    "4. View and analyze the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open this notebook in Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click link to open notebook in google colab:  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IBM/data-prep-kit/blob/8381d353c3aa90d334e81ac3029ab774c753cc4b/examples/notebooks/hap/generate_hap_score_csv.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies for Google Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install data-prep-connector\n",
    "! pip install  'data-prep-toolkit[ray]==0.2.2.dev1'\n",
    "! pip install  'data-prep-toolkit-transforms[ray,all]==0.2.2.dev1'\n",
    "! pip install nltk==3.9.1 transformers==4.38.2 torch>=2.2.2,<=2.4.1 pandas==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Paths\n",
    "---------------------\n",
    "Define the paths for the script, input folder, and output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path().resolve()\n",
    "relative_script_dir = '../../../transforms/universal/hap/python/src/hap_local_python.py'\n",
    "hap_script_path = (notebook_dir / relative_script_dir).resolve()\n",
    "\n",
    "input_folder = \"./input\"\n",
    "output_folder = \"./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the necessary folders exist.\n",
    "os.makedirs(input_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "print(f\"Script Path: {hap_script_path}\")\n",
    "print(f\"Input Folder: {input_folder}\")\n",
    "print(f\"Output Folder: {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Check for CSV Files in Input Folder\n",
    "\n",
    "- Place your CSV file in the `input_folder`.\n",
    "- Ensure the column containing the text matches the `doc_text_column` parameter.\n",
    "- If your text column has a different name, update the `doc_text_column` parameter in later cells.\n",
    "- This cell sets up the file paths for the input file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"No CSV files found in the input folder: {input_folder}\")\n",
    "    print(\"Please place a CSV file in the input folder and rerun this notebook.\")\n",
    "else:\n",
    "    print(f\"Found CSV file(s): {csv_files}\")\n",
    "\n",
    "# Pick the first CSV file in the folder\n",
    "csv_file_path = os.path.join(input_folder, csv_files[0])\n",
    "print(f\"Using CSV file: {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert CSV to Parquet\n",
    "Convert the selected CSV file to Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_path = os.path.join(input_folder, \"data.parquet\")\n",
    "df = pd.read_csv(csv_file_path)\n",
    "df.to_parquet(parquet_file_path, index=False)\n",
    "print(f\"CSV file converted to Parquet format at: {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define HAP Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hap_params = {\n",
    "    \"model_name_or_path\": \"ibm-granite/granite-guardian-hap-38m\",  # Default model name\n",
    "    \"annotation_column\": \"hap_score\",  # Output column for HAP scores\n",
    "    \"doc_text_column\": \"Customer Feedback\",  # Input column containing document text\n",
    "    \"inference_engine\": \"CPU\",  # Inference engine (CPU or GPU)\n",
    "    \"max_length\": 512,  # Maximum token length\n",
    "    \"batch_size\": 128,  # Batch size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run the Transform with defined HAP Paramters\n",
    "\n",
    "This cell executes the HAP transformation script:\n",
    "- `--input_file`: Path to your input CSV/Parquet file.\n",
    "- `--output_file`: Path where the output file with HAP scores will be saved.\n",
    "- `--doc_text_column`: The column containing the text for analysis (default: `Customer Feedback`).\n",
    "- `--annotation_column`: The column where HAP scores will be saved (default: `hap_score`).\n",
    "\n",
    "**Customization**: \n",
    "- If your text column has a different name, update the value of `--doc_text_column` accordingly.\n",
    "- You can adjust other parameters like `--batch_size` and `--max_length` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the current environment variables\n",
    "env = os.environ.copy()\n",
    "\n",
    "# Set Environment Variables for HAP Parameters\n",
    "os.environ[\"MODEL_NAME_OR_PATH\"] = \"ibm-granite/granite-guardian-hap-38m\"\n",
    "os.environ[\"ANNOTATION_COLUMN\"] = \"hap_score\"\n",
    "os.environ[\"DOC_TEXT_COLUMN\"] = \"Customer Feedback\"\n",
    "os.environ[\"INFERENCE_ENGINE\"] = \"CPU\"\n",
    "os.environ[\"MAX_LENGTH\"] = \"512\"\n",
    "os.environ[\"BATCH_SIZE\"] = \"128\"\n",
    "os.environ[\"INPUT_FOLDER\"] = input_folder\n",
    "os.environ[\"OUTPUT_FOLDER\"] = output_folder\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", hap_script_path],\n",
    "        check=True,\n",
    "        text=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "\n",
    "    # If successful, print the result of the transform\n",
    "    print(\"Transform completed successfully.\")\n",
    "    print(result.stdout)\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    # If there was an error, print the error message\n",
    "    print(\"Error occurred during transform execution.\")\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generate the Output CSV\n",
    "\n",
    "This step checks for any existing CSV files in the output folder and removes them before generating new ones. The following actions are performed:\n",
    "\n",
    "1. **Listing Output Files**: The script lists all files in the output folder.\n",
    "2. **Check for Parquet Files**: It identifies `.parquet` files in the output folder.\n",
    "3. **Remove Old CSV Files**: If any previous output files (`hap_complete_output.csv` or `hap_filtered_output.csv`) exist, they are deleted.\n",
    "4. **Read Parquet File**: The Parquet file is read into a DataFrame.\n",
    "5. **Filter Data**: The relevant columns, `doc_text_column` (from the environment variable) and `hap_score_column`, are selected from the DataFrame.\n",
    "6. **Save New CSV Files**: The filtered data is saved into two new CSV files:\n",
    "   - `hap_complete_output.csv` (containing the full output)\n",
    "   - `hap_filtered_output.csv` (containing only the filtered relevant columns).\n",
    "\n",
    "This ensures that only the latest output is retained, and no old files remain in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List all files in the output folder\n",
    "output_files = os.listdir(output_folder)\n",
    "\n",
    "if output_files:\n",
    "    for file in output_files:\n",
    "        if file.endswith(\".parquet\"):  # Check for Parquet output files\n",
    "            output_file_path = os.path.join(output_folder, file)\n",
    "            output_df = pd.read_parquet(output_file_path)  # Read the Parquet file\n",
    "            print(f\"Complete Output Parquet File Path: {output_file_path}\")\n",
    "\n",
    "            # Define the output CSV file paths\n",
    "            complete_output_csv = os.path.join(output_folder, \"hap_complete_output.csv\")\n",
    "            filtered_output_csv = os.path.join(output_folder, \"hap_filtered_output.csv\")\n",
    "\n",
    "            # Remove old CSV files if they exist\n",
    "            if os.path.exists(complete_output_csv):\n",
    "                os.remove(complete_output_csv)\n",
    "                print(f\"Old complete CSV file removed: {complete_output_csv}\")\n",
    "\n",
    "            if os.path.exists(filtered_output_csv):\n",
    "                os.remove(filtered_output_csv)\n",
    "                print(f\"Old filtered CSV file removed: {filtered_output_csv}\")\n",
    "\n",
    "            # Filter the output DataFrame to only include the relevant columns\n",
    "            hap_score_column = hap_params[\"annotation_column\"]\n",
    "            doc_text_column = os.getenv('DOC_TEXT_COLUMN')  # Read from environment variable\n",
    "            filtered_df = output_df[[doc_text_column, hap_score_column]]\n",
    "\n",
    "            # Print the filtered DataFrame (only showing the HAP score and document text)\n",
    "            print(f\"Filtered Output (only HAP score and document text):\")\n",
    "            display(filtered_df)\n",
    "\n",
    "            # Save the complete output as a CSV file\n",
    "            output_df.to_csv(complete_output_csv, index=False)  # Convert the Parquet to CSV\n",
    "            print(f\"Complete output saved to: {complete_output_csv}\")\n",
    "\n",
    "            # Save the filtered output as a CSV file\n",
    "            filtered_df.to_csv(filtered_output_csv, index=False)\n",
    "            print(f\"Filtered output saved to: {filtered_output_csv}\")\n",
    "\n",
    "else:\n",
    "    print(\"No output files found. Please check the script or configuration.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataprepkit",
   "language": "python",
   "name": "data-prep-kit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
