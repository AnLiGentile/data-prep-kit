{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl a website \n",
    "\n",
    "We will use `DPK-Connector` module\n",
    "\n",
    "References\n",
    "- https://github.com/IBM/data-prep-kit/tree/dev/data-connector-lib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleared output directory\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DIR ):\n",
    "    shutil.os.makedirs(MY_CONFIG.INPUT_DIR, exist_ok=True)\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_DIR, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print (\"✅ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-3: Crawl the website\n",
    "\n",
    "We will use `dpk-connector` utility to download a some HTML pages from a site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://thealliance.ai/\n",
      "url: https://thealliance.ai/core-projects/ntia_request\n",
      "url: https://thealliance.ai/contact\n",
      "url: https://thealliance.ai/blog/open-source-ai-demo-night-sf-2024\n",
      "url: https://thealliance.ai/core-projects/trusted-evals\n",
      "url: https://thealliance.ai/core-projects/sb1047\n",
      "url: https://thealliance.ai/focus-areas/hardware-enablement\n",
      "url: https://thealliance.ai/focus-areas/applications-and-tools\n",
      "url: https://thealliance.ai/focus-areas/trust-and-safety\n",
      "url: https://thealliance.ai/community\n",
      "url: https://thealliance.ai/focus-areas/advocacy\n",
      "url: https://thealliance.ai/focus-areas/skills-education\n",
      "url: https://thealliance.ai/contribute\n",
      "url: https://thealliance.ai/become-a-collaborator\n",
      "url: https://thealliance.ai/focus-areas/foundation-models-datasets\n",
      "url: https://thealliance.ai/aia-members\n",
      "url: https://thealliance.ai/our-work\n",
      "url: https://thealliance.ai/about-aia\n",
      "url: https://thealliance.ai/core-projects/the-living-guide-to-applying-ai\n",
      "url: https://thealliance.ai/core-projects/trust-and-safety-evaluations\n",
      "url: https://thealliance.ai/core-projects/safety-priorities-ranking-by-domain\n",
      "url: https://thealliance.ai/core-projects/trust-user-guide\n",
      "url: https://thealliance.ai/blog/ai-alliance-expands-with-seven-new-members-from-in\n",
      "url: https://thealliance.ai/blog/ai-for-drug-discovery-open-innovation-forum\n",
      "url: https://thealliance.ai/blog/unesco-language-translator\n",
      "url: https://thealliance.ai/blog/advancing-domain-specific-qa-the-ai-alliances-guid\n",
      "url: https://thealliance.ai/core-projects/ai-accelerator-software-ecosystem-guide\n",
      "url: https://thealliance.ai/blog\n",
      "url: https://thealliance.ai/blog/evaluation-of-generative-ai---whats-ultimately-our\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Crawl is done'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dpk_connector import crawl, shutdown\n",
    "import nest_asyncio\n",
    "import os\n",
    "from my_utils import get_mime_type, get_filename_from_url\n",
    "from dpk_connector.core.utils import validate_url\n",
    "\n",
    "# Use nest_asyncio to enable a nested event loop run for the crawler inside the Jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize counter\n",
    "retrieved_pages = 0\n",
    "saved_pages = 0\n",
    "\n",
    "# Define a callback function to be executed at the retrieval of each page during a crawl\n",
    "def on_downloaded(url: str, body: bytes, headers: dict) -> None:\n",
    "    \"\"\"\n",
    "    Callback function called when a page has been downloaded.\n",
    "    You have access to the request URL, response body and headers.\n",
    "    \"\"\"\n",
    "    global retrieved_pages, saved_pages\n",
    "    retrieved_pages+=1\n",
    "    \n",
    "    print (f'on_downloaded (url={url})')\n",
    "\n",
    "    if saved_pages < MY_CONFIG.CRAWL_MAX_DOWNLOADS:\n",
    "        print(f\"Visited url: {url}\")\n",
    "\n",
    "    # Get mime_type of retrieved page\n",
    "    mime_type = get_mime_type(body)\n",
    "    print (f'mime_type={mime_type}')\n",
    "    \n",
    "    # Save the page if it is a PDF to only download research papers\n",
    "    if MY_CONFIG.CRAWL_MIME_TYPE  in mime_type.lower():\n",
    "        print ('hi')\n",
    "        filename = get_filename_from_url(url)\n",
    "        local_file_path = os.path.join(MY_CONFIG.INPUT_DIR, filename)\n",
    "        print (local_file_path)\n",
    "        \n",
    "        with open(local_file_path, 'wb') as f:\n",
    "            f.write(body)\n",
    "            \n",
    "        if saved_pages < MY_CONFIG.CRAWL_MAX_DOWNLOADS :\n",
    "            print(f\"Saved contents of url: {url}\")\n",
    "        saved_pages+=1\n",
    "        \n",
    "# Define a user agent to provide information about the client making the request\n",
    "# user_agent = \"dpk-connector\"\n",
    "user_agent = \"Mozilla/5.0 (X11; Linux i686; rv:125.0) Gecko/20100101 Firefox/125.0\"\n",
    "\n",
    "\n",
    "def on_downloaded2(url: str, body: bytes, headers: dict) -> None:\n",
    "    # print(f\"url: {url}, headers: {headers}, body: {body[:64]}\")\n",
    "    print(f\"url: {url}\")\n",
    "\n",
    "async def run_my_crawl():\n",
    "    crawl([MY_CONFIG.CRAWL_URL_BASE], \n",
    "          on_downloaded2,  \n",
    "          user_agent=user_agent, \n",
    "          depth_limit = MY_CONFIG.CRAWL_MAX_DEPTH, \n",
    "          subdomain_focus=True,\n",
    "        #   path_focus = True, \n",
    "          download_limit = MY_CONFIG.CRAWL_MAX_DOWNLOADS)\n",
    "    return \"Crawl is done\"\n",
    "\n",
    "# Now run the configured crawl\n",
    "await run_my_crawl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpk-6-basic-022dev2-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
