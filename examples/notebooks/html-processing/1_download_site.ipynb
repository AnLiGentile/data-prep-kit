{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleared output directory\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DIR ):\n",
    "    shutil.os.makedirs(MY_CONFIG.INPUT_DIR, exist_ok=True)\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_DIR, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print (\"✅ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a Website\n",
    "\n",
    "We will use `dpk-connector` utility to download a some HTML pages from a site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visited url: https://thealliance.ai/our-work\n",
      "input/our-work\n",
      "Saved contents of url: https://thealliance.ai/our-work\n",
      "Visited url: https://thealliance.ai/affiliated-projects/quansight_ragna\n",
      "input/quansight_ragna\n",
      "Saved contents of url: https://thealliance.ai/affiliated-projects/quansight_ragna\n",
      "Visited url: https://thealliance.ai/affiliated-projects/cornell_generative\n",
      "input/cornell_generative\n",
      "Saved contents of url: https://thealliance.ai/affiliated-projects/cornell_generative\n",
      "Visited url: https://thealliance.ai/core-projects/trust-and-safety-evaluations\n",
      "input/trust-and-safety-evaluations\n",
      "Saved contents of url: https://thealliance.ai/core-projects/trust-and-safety-evaluations\n",
      "Visited url: https://thealliance.ai/affiliated-projects/mbzuai_llm\n",
      "input/mbzuai_llm\n",
      "Saved contents of url: https://thealliance.ai/affiliated-projects/mbzuai_llm\n",
      "Visited url: https://thealliance.ai/affiliated-projects/trusty_ai\n",
      "input/trusty_ai\n",
      "Saved contents of url: https://thealliance.ai/affiliated-projects/trusty_ai\n",
      "Visited url: https://thealliance.ai/core-projects/time-series-data-and-model-initiative\n",
      "input/time-series-data-and-model-initiative\n",
      "Saved contents of url: https://thealliance.ai/core-projects/time-series-data-and-model-initiative\n",
      "Visited url: https://thealliance.ai/core-projects/open-trusted-data-initiative\n",
      "input/open-trusted-data-initiative\n",
      "Saved contents of url: https://thealliance.ai/core-projects/open-trusted-data-initiative\n",
      "Visited url: https://thealliance.ai/core-projects/safety-priorities-ranking-by-domain\n",
      "input/safety-priorities-ranking-by-domain\n",
      "Saved contents of url: https://thealliance.ai/core-projects/safety-priorities-ranking-by-domain\n",
      "Visited url: https://thealliance.ai/core-projects/ntia_request\n",
      "input/ntia_request\n",
      "Saved contents of url: https://thealliance.ai/core-projects/ntia_request\n",
      "input/aiconfig_editor\n",
      "Saved contents of url: https://thealliance.ai/affiliated-projects/aiconfig_editor\n",
      "input/ibm_granite\n",
      "Saved contents of url: https://thealliance.ai/affiliated-projects/ibm_granite\n",
      "input/the-living-guide-to-applying-ai\n",
      "Saved contents of url: https://thealliance.ai/core-projects/the-living-guide-to-applying-ai\n",
      "input/trusted-evals\n",
      "Saved contents of url: https://thealliance.ai/core-projects/trusted-evals\n",
      "input/sb1047\n",
      "Saved contents of url: https://thealliance.ai/core-projects/sb1047\n",
      "input/ai-accelerator-software-ecosystem-guide\n",
      "Saved contents of url: https://thealliance.ai/core-projects/ai-accelerator-software-ecosystem-guide\n",
      "input/trust-user-guide\n",
      "Saved contents of url: https://thealliance.ai/core-projects/trust-user-guide\n",
      "input/contact\n",
      "Saved contents of url: https://thealliance.ai/contact\n",
      "input/industry-open-fms-initiative\n",
      "Saved contents of url: https://thealliance.ai/core-projects/industry-open-fms-initiative\n",
      "input/become-a-collaborator\n",
      "Saved contents of url: https://thealliance.ai/become-a-collaborator\n",
      "input/affiliated-projects\n",
      "input/advocacy\n",
      "input/community\n",
      "input/foundation-models-datasets\n",
      "input/hardware-enablement\n",
      "input/foundation-models\n",
      "input/contribute\n",
      "input/applications-and-tools\n",
      "input/aia-members\n",
      "input/about-aia\n",
      "input/skills-education\n",
      "input/trust-and-safety\n",
      "input/blog\n",
      "input/core-projects\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Crawl is done'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dpk_connector import crawl, shutdown\n",
    "import nest_asyncio\n",
    "import os\n",
    "from my_utils import get_mime_type, get_filename_from_url\n",
    "from dpk_connector.core.utils import validate_url\n",
    "\n",
    "# Use nest_asyncio to enable a nested event loop run for the crawler inside the Jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize counter\n",
    "retrieved_pages = 0\n",
    "saved_pages = 0\n",
    "\n",
    "# Define a callback function to be executed at the retrieval of each page during a crawl\n",
    "def on_downloaded(url: str, body: bytes, headers: dict) -> None:\n",
    "    \"\"\"\n",
    "    Callback function called when a page has been downloaded.\n",
    "    You have access to the request URL, response body and headers.\n",
    "    \"\"\"\n",
    "    global retrieved_pages, saved_pages\n",
    "    retrieved_pages+=1\n",
    "\n",
    "    if saved_pages < MY_CONFIG.CRAWL_MAX_DOWNLOADS:\n",
    "        print(f\"Visited url: {url}\")\n",
    "\n",
    "    # Get mime_type of retrieved page\n",
    "    mime_type = get_mime_type(body)\n",
    "    \n",
    "    # Save the page if it is a PDF to only download research papers\n",
    "    if MY_CONFIG.CRAWL_MIME_TYPE  in mime_type.lower():\n",
    "        filename = get_filename_from_url(url)\n",
    "        local_file_path = os.path.join(MY_CONFIG.INPUT_DIR, filename)\n",
    "        print (local_file_path)\n",
    "        \n",
    "        with open(local_file_path, 'wb') as f:\n",
    "            f.write(body)\n",
    "            \n",
    "        if saved_pages < MY_CONFIG.CRAWL_MAX_DOWNLOADS :\n",
    "            print(f\"Saved contents of url: {url}\")\n",
    "        saved_pages+=1\n",
    "        \n",
    "# Define a user agent to provide information about the client making the request\n",
    "user_agent = \"dpk-connector\"\n",
    "\n",
    "async def run_my_crawl():\n",
    "    crawl([MY_CONFIG.CRAWL_URL_BASE], \n",
    "          on_downloaded,  \n",
    "          user_agent=user_agent, \n",
    "          depth_limit = MY_CONFIG.CRAWL_MAX_DEPTH, \n",
    "          path_focus = True, \n",
    "          download_limit = MY_CONFIG.CRAWL_MAX_DOWNLOADS)\n",
    "    return \"Crawl is done\"\n",
    "\n",
    "# Now run the configured crawl\n",
    "await run_my_crawl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpk-3-r022dev2-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
